{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Second Year Project\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning goals**\n",
    "\n",
    "- be familiar with the concept of regular expressions\n",
    "- be able to discuss issues that arise in tokenization and segmentation\n",
    "- be able to use the Unix command-line tools for navigation, search (`grep`), count (`wc`), and basic text processing (`sed` for substitution), as well as the pipe (`|`), e.g., to count word types or extract a simple word frequency list\n",
    "\n",
    "**Notebook overview**\n",
    "\n",
    "*Lecture 1*\n",
    "1. Regular expressions - get basic familiarity with the concept\n",
    "2. Tokenization - learn about how to approach tokenization and its challenges, then apply the knowledge to a small example text\n",
    "3. Twitter tokenization - learn how to tokenize domain specific text\n",
    "4. Sentence segmentation - learn how to segment given text into sentences\n",
    "\n",
    "*Lecture 2*\n",
    "\n",
    "5. Linux Command Line for NLP - learn how to use command line to quickly extract useful information from provided text file\n",
    "6. Advanced Use of Linux Command Line - construct more complex command to extract word frequency from a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Regular Expressions (pen and paper)\n",
    "For this section, it might be handy to use the website https://regex101.com/ to test your solutions.\n",
    "Note: By word, we mean any alphabetic string separated from other words by whitespace, any relevant punctuation, line breaks, etc., as defined in [J&M](https://web.stanford.edu/~jurafsky/slp3/old_dec21/). If we do not specify word, any substring match might be sufficient.\n",
    "- a) Write a regular expression (regex or pattern) that matches any of the following words: `cat`, `sat`, `mat`.\n",
    "<br>\n",
    "(Bonus: What is a possible long solution? Can you find a shorter solution? hint: match characters instead of words)\n",
    "- b) Write a regular expression that matches numbers, e.g. 12, 1,000, 39.95\n",
    "- c) Expand the previous solution to match Danish prices indications, e.g., `1,000 kr` or `39.95 DKK` or `19.95`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "(Adapted notebook from S. Riedel, UCL & Facebook: https://github.com/uclnlp/stat-nlp-book).\n",
    "\n",
    "In Python, a simple way to tokenize a text is via the `split` method that divides a text wherever a particular substring is found. In the code below this pattern is simply the whitespace character, and this seems like a reasonable starting point for an English tokenization approach.\n",
    "\n",
    "This is analogous to the `sed` command we have seen in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan.\\nWhy',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Mr. Bob Dobolina is thinkin' of a master plan.\" + \\\n",
    "       \"\\nWhy doesn't he quit?\"\n",
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make more fine-grained decision, we will focus on using regular expressions for tokenization in this assignment. This can be done by either:\n",
    "1. Defining the character sequence patterns at which to split.\n",
    "2. Specifying patters that define what constitutes a token. \n",
    "\n",
    "In the code below we use a simple pattern `\\s` that matches **any whitespace** to define where to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan.',\n",
       " 'Why',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "gap = re.compile('\\s')\n",
    "gap.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **shortcoming** of this tokenization is its treatment of punctuation because it considers `plan.` as a token whereas ideally we would prefer `plan` and `.` to be distinct tokens. It might be easier to address this problem if we define what a token is, instead of what constitutes a gap. Below we have defined tokens as sequences of alphanumeric characters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr',\n",
       " '.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " 'thinkin',\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan',\n",
       " '.',\n",
       " 'Why',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'he',\n",
       " 'quit',\n",
       " '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = re.compile('\\w+|[.?:]')\n",
    "token.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still isn't perfect as `Mr.` is split into two tokens, but it should be a single token. Moreover, we have actually lost an apostrophe. Both are fixed below, although we now fail to break up the contraction `doesn't`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan',\n",
       " '.',\n",
       " 'Why',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit',\n",
       " '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = re.compile('Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we have an input text and apply the tokenizer (described previously) on the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Curiouser\", 'and', 'curiouser', \"'\", 'cried', 'Alice', 'she', 'was', 'so', 'much']\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"'Curiouser and curiouser!' cried Alice (she was so much surprised, that for the moment she quite\n",
    "forgot how to speak good English); 'now I'm opening out like the largest telescope that ever was! Good-bye,\n",
    "feet!' (for when she looked down at her feet, they seemed to be almost out of sight, they were getting so far\n",
    "off). 'Oh, my poor little feet, I wonder who will put on your shoes and stockings for you now, dears? I'm sure I\n",
    "shan't be able! I shall be a great deal too far off to trouble myself about you: you must manage the best\n",
    "way you can; â€”but I must be kind to them,' thought Alice, 'or perhaps they won't walk the way I want to go!\n",
    "Let me see: I'll give them a new pair of boots every Christmas...'\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile('Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "print(tokens[:10])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* a) The tokenizer clearly makes a few mistakes. Where?\n",
    "\n",
    "* b) Write a tokenizer to correctly tokenize the text.\n",
    "\n",
    "* c) Should one separate `'m`, `'ll`, `n't`, possessives, and other forms of contractions from the word? Implement a tokenizer that separates these, and attaches the `'` to the latter part of the contraction.\n",
    "\n",
    "* d) Should elipsis (...) be considered as three `.`s or one `...`? Design a regular expression for both solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Twitter Tokenization\n",
    "As you might imagine, tokenizing tweets differs from standard tokenization. There are 'rules' on what specific elements of a tweet might be (mentions, hashtags, links), and how they are tokenized. The goal of this exercise is not to create a bullet-proof Twitter tokenizer but to understand tokenization in a different domain.\n",
    "\n",
    "In the next exercises, we will focus on the following tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@robv New vids coming tomorrow #excited_as_a_child, can't w8!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robv', 'New', 'vids', 'coming', 'tomorrow', 'excited_as_a_child', 'can', 't', 'w8']\n"
     ]
    }
   ],
   "source": [
    "token = re.compile('[\\w]+')\n",
    "tokens = token.findall(tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) What is the correct tokenization of the tweet above according to you?\n",
    "- b) Try your tokenizer from the previous exercise (Question 4). Which cases are going wrong? Make sure your tokenizer handles the above tweet correctly.\n",
    "- c) Will your tokenizer correctly tokenize emojis?\n",
    "- d) Think of at least one other example where your tokenizer will behave incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation is not a trivial task either.\n",
    "\n",
    "First, make sure you understand the following sentence segmentation code used in the lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_segment(match_regex, tokens):\n",
    "    \"\"\"\n",
    "    Splits a sequence of tokens into sentences, splitting wherever the given matching regular expression\n",
    "    matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    match_regex the regular expression that defines at which token to split.\n",
    "    tokens the input sequence of string tokens.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of token lists, where each inner list represents a sentence.\n",
    "\n",
    "    >>> tokens = ['the','man','eats','.','She', 'sleeps', '.']\n",
    "    >>> sentence_segment(re.compile('\\.'), tokens)\n",
    "    [['the', 'man', 'eats', '.'], ['She', 'sleeps', '.']]\n",
    "    \"\"\"\n",
    "    current = []\n",
    "    sentences = [current]\n",
    "    for tok in tokens:\n",
    "        current.append(tok)\n",
    "        if match_regex.match(tok):\n",
    "            current = []\n",
    "            sentences.append(current)\n",
    "    if not sentences[-1]:\n",
    "        sentences.pop(-1)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, there is a variable `text` containing a small text and a regular expression-based segmenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U', '.']\n",
      "['K', '.']\n",
      "[\"Isn't\", 'that', 'weird', '?', 'I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?', 'Of', 'course', 'U', '.']\n",
      "['S', '.']\n",
      "['A', '.']\n",
      "['also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http', 'www', '.']\n",
      "['wikipedia', '.']\n",
      "['org', 'during', 'their', 'Ph', '.']\n",
      "['D', '.']\n",
      "['or', 'an', 'M', '.']\n",
      "['Sc', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch is the longest official one-word placename in U.K. Isn't that weird? I mean, someone took the effort to really make this name as complicated as possible, huh?! Of course, U.S.A. also has its own record in the longest name, albeit a bit shorter... This record belongs to the place called Chargoggagoggmanchauggagoggchaubunagungamaugg. There's so many wonderful little details one can find out while browsing http://www.wikipedia.org during their Ph.D. or an M.Sc.\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile('Mr.|[\\w\\']+|[.?]+')\n",
    "\n",
    "tokens = token.findall(text)\n",
    "sentences = sentence_segment(re.compile('\\.'), tokens)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) Improve the segmenter so that it segments the text in the way you think it is correct.\n",
    "- b) How would you deal with all URLs effectively?\n",
    "- c) Can you think of other problematic cases not covered in the example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linux Command Line for NLP: Conll Format\n",
    "In natural language processing, the \"conll\" format is a highly common standard to represent annotated text. There is a variety of conll formats, so it might be more correct to refer to them as conll-like formats. These formats have one word per line, separate sentences with an empty line, and have separate collumns (separated with tabs) for each annotation layer.\n",
    "\n",
    "In this assignment, we will use the conll format for named entity recognition (from conll2002: [paper](https://aclanthology.org/W02-2024.pdf)). We will use Danish data from (DaN+)[https://aclanthology.org/2020.coling-main.583.pdf]. This data follows the BIO labels as discussed in the lecture. An example of the data is shown below, this example has one entity-span \"goergh bush\":\n",
    "\n",
    "```\n",
    "-       O\n",
    "en      O\n",
    "mand    O\n",
    "der     O\n",
    "hedder  O\n",
    "goergh  B-PER\n",
    "bush    I-PER\n",
    ".       O\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Use Unix command line tools for this assignment (grep, sed, etc.)\n",
    "\n",
    "* a) Search in the `da_arto.conll` file (in the assingment1 directory) for first names. You can assume that first names always have the label B-PER, and that the string \"B-PER\" does not occur in the first column. \n",
    "* b) How many names occur in the data?\n",
    "* c) How can we make sure that we do not match the string \"B-PER\" occuring in the first column?\n",
    "* d) How can we clean away the labels, so that we have only a list of names left? (hint: pipe the result of the previous command into a `split`)\n",
    "* e) How many of the names you found start with an uppercased character?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. More Advanced Usage of Unix Tools: Creating a word frequency list, finding function words\n",
    "Let us now create a simple word frequency list from the book above using Unix tools to answer the following question: Which four (function) words are the most frequent in *The Adventures of Sherlock Holmes* by Arthur Conan Doyle (`pg1661.txt`)?\n",
    "\n",
    "* The first step is to split the text into separate words. Here, we will use the command sed to replace all spaces with a newline:\n",
    "\n",
    "```\n",
    "sed 's/ /\\n/g' FILE\n",
    "```\n",
    "\n",
    "Note: Remember the flag `g`, which stands for global. It replaces all occurrences of a space on a line.\n",
    "\n",
    "* Hint: It is handy to forward this command to a tool called `less`, which lets you browse through the result (type `q` to quit).\n",
    "\n",
    "```\n",
    "sed 's/ /\\n/g' FILE | less\n",
    "```\n",
    "\n",
    "* Now we can sort the list of tokens and count unique words:\n",
    "\n",
    "```\n",
    "sed 's/ /\\n/g' FILE | sort | uniq -c\n",
    "```\n",
    "\n",
    "* To create the most frequent words first, sort again in reverse numeric order (find the options of `sort` to do so, e.g. check `man sort`).\n",
    "\n",
    "Note: Here we used `sed`, our textbook shows an alternative with `tr` instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
