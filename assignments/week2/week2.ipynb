{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Second Year Project\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning goals**\n",
    "* be familiar with the Universal POS tagset and data annotation for POS tagging\n",
    "* be able to discuss annotation quality, both qualitatively and quantitatively, by comparing your annotations to those of a peer\n",
    "* be able to implement a Naive Bayes and a  logistic regression classifier for language identification, using BOW and character n-gram features\n",
    "* analyze the performance of classifiers both on in-domain and out-of-domain data.\n",
    "\n",
    "**Notebook overview**\n",
    "\n",
    "*Lecture 3*\n",
    "1. Annotation - annotate a small sample of social media data with POS tags\n",
    "2. Annotation quality - inspect annotation quality through kappa scores, but also qualitatively\n",
    "3. Words as features - convert input text to features that can be used in machine learning algorithms\n",
    "\n",
    "*Lecture 4*\n",
    "\n",
    "4. Naive Bayes Classifier (pen and paper)\n",
    "5. Naive Bayes with BOW in sklearn - train a classifier with bag-of-word features\n",
    "6. Discriminative Classifier with BOW - train a discriminative classifier\n",
    "7. Character n-grams - extract and use character n-grams\n",
    "8. Analysis of a model's performance - some examples of how to analyze when/how a model fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Annotation and POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment consists of 2 parts: first you annotate the data, then you compare your annotations against the annotations of a peer. For this reason, we have an intermediate deadline for uploading your annotations on learnit:\n",
    "* **08-02-2022** 12:00 Danish time: upload your annotations on learnit\n",
    "\n",
    "Before Thursday (09-02) you will receive the annotation from a peer for the same data. You can then compare the annotations. Note that you can already implement your solution on tuesday, and change some of your own annotation to use them as a dummy test-file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Annotation \n",
    "\n",
    "Find the file with your ITU username in `assignments/week2/annotate/`. In this file, you will find 20\n",
    "TikTok comments which are pre-tokenized and in conll format (see [week1](https://github.itu.dk/robv/intro-nlp/blob/main/assignments/week1/week1.ipynb)). Behind each word you are supposed to annotate the pos tag, with one tab in between. The final file should look like this:\n",
    "\n",
    "```\n",
    "-       PUNCT\n",
    "en      DET\n",
    "mand    NOUN\n",
    "der     PRON\n",
    "hedder  VERB\n",
    "goergh  PROPN\n",
    "bush    PROPN\n",
    ".       PUNCT\n",
    "\n",
    "```\n",
    "\n",
    "You can use a whitespace or a tab between the word and its tag. Please check with the script posCheck.py\n",
    "whether the file format is correct. Usage: `python3 posCheck.py origFile annotatedFile`\n",
    "For annotation guidelines we refer to the slides and https://universaldependencies.org/u/pos/all.html. Alternatively, it might be helpful to look at example annotations, which are provided in:\n",
    "`assignments/week2/pos-data/da_ddt-ud-sample.conllu` and `assignments/week2/pos-data/en_ewt-ud-sample.conllu`\n",
    "\n",
    "**NOTE** If you do not speak Danish, please annotate the English sample (ending with _en)\n",
    "\n",
    "Upload your annotation on LearnIT (before **08-02-2022 12:00**), and name it like: `[username]_[language].conll` if your username is robv for example, use: `robv_da.conll`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Annotation Quality\n",
    "\n",
    "* a) Calculate the accuracy between you and the other annotator, how often did you agree?\n",
    "* b) Now implement Cohen’s Kappa score, and calculate the Kappa for your annotation sample. In which range\n",
    "does your Kappa score fall?\n",
    "* c) Take a closer look at the cases where you disagreed with the other annotator; are these disagreements due\n",
    "to ambiguity, or are there mistakes in the annotation? Would you classify your agreement in the same category as it falls in the standard kappa interpretation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Words as Features\n",
    "In this assignment, we will convert a text to a matrix of features for the purpose of language identification (the classifiers will be trained in thursdays assignments, see below). We will use data from star-wars fandom wikipedia:\n",
    "* English [Wookipedia](https://starwars.fandom.com/wiki/Main_Page)  \n",
    "* Danish [Kraftens Arkiver](https://starwars.fandom.com/da/wiki) \n",
    "* Dutch [Yodapedia](https://starwars.fandom.com/da/wiki)\n",
    "\n",
    "The data for the following assignments can be read like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_langid(path):\n",
    "    text = []\n",
    "    labels = []\n",
    "    for line in open(path):\n",
    "        tok = line.strip().split('\\t')\n",
    "        labels.append(tok[0])\n",
    "        text.append(tok[1])\n",
    "    return text, labels\n",
    "\n",
    "wooki_train_text, wooki_train_labels = load_langid('langid-data/wookipedia_langid.train.tok.txt')\n",
    "wooki_dev_text, wooki_dev_labels = load_langid('langid-data/wookipedia_langid.dev.tok.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Convert the train data to \"binary word features\". This means that every instance (sentence) is represented by a vector of binary values, each of which correspongs to a word. For example (features are on the columns, input on the rows):\n",
    "\n",
    "|             | hello | bye | there | here | ... |\n",
    "|-------------|-------|-----|-------|------|-----|\n",
    "| hello there | 1     | 0   | 1     | 0    |     |\n",
    "| bye bye     | 0     | 1   | 0     | 0    |     |\n",
    "\n",
    "\n",
    "Note that this means that you will end up with a matrix of size `(#data_instances, len(vocab))` where `vocab` contains your vocabulary (i.e. all the words in the train data), and the `#data_instances` is the number of input sentences (feel free to use numpy, torch or native python lists). This matrix will be filled with 0's and 1's, indicating which features are present in which instances.\n",
    "\n",
    "**Hint**: Start with two sentences, as it is much easier to debug (and with 1 sentence, you will have only 1s)\n",
    "\n",
    "b) Convert the dev data to the same features generated from the training data. Note that no new features can be introduced at this point, check whether the size of the matrix is `(#dev_instances, len(vocab))`.\n",
    "\n",
    "c) Write down what are the pros and cons of representing text as `BOW` (bag-of-words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Generative and Discriminative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes Classifier (pen and paper)\n",
    "\n",
    "Solve the following exercises from [Chapter 4 of Speech and Language processing](https://web.stanford.edu/~jurafsky/slp3/4.pdf):\n",
    "\n",
    "a) Exercise 4.1 from J&M: (copied here for your convenience):\n",
    "\n",
    "Assume the following likelihoods for each word being part of a positive or\n",
    "negative movie review, and equal prior probabilities for each class.\n",
    "\n",
    "| feature         | pos | neg     |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| I      |  0.09      |  0.16  |\n",
    "| always   | 0.07        | 0.06      |\n",
    "| like      | 0.29       | 0.06   |\n",
    "| foreign      | 0.04       | 0.15   |\n",
    "| films      |  0.08      | 0.11   |\n",
    "\n",
    "- What class will Naive Bayes assign to the sentence `“I always like foreign films.”`?\n",
    "\n",
    "b) Exercise 4.2 from J&M (copied here for your convenience):\n",
    "\n",
    "Given the following short movie reviews, each labeled with a genre, either comedy or action:\n",
    "\n",
    "1. fun, couple, love, love **comedy**\n",
    "\n",
    "2. fast, furious, shoot **action**\n",
    "\n",
    "3. couple, fly, fast, fun, fun **comedy**\n",
    "\n",
    "4. furious, shoot, shoot, fun **action**\n",
    "\n",
    "5. fly, fast, shoot, love **action**\n",
    "\n",
    "and a new document D:\n",
    "\n",
    "```\n",
    "fast, couple, shoot, fly\n",
    "```\n",
    "\n",
    "- Compute the most likely class for D. Assume a Naive Bayes classifier and use *add-1 smoothing* for the likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes with BOW in sklearn \n",
    "\n",
    "In this assignment, we will focus on the task of language identification. You can use the data from assignment 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_langid(path):\n",
    "    text = []\n",
    "    labels = []\n",
    "    for line in open(path):\n",
    "        tok = line.strip().split('\\t')\n",
    "        labels.append(tok[0])\n",
    "        text.append(tok[1])\n",
    "    return text, labels\n",
    "\n",
    "wooki_train_text, wooki_train_labels = load_langid('langid-data/wookipedia_langid.train.tok.txt')\n",
    "wooki_dev_text, wooki_dev_labels = load_langid('langid-data/wookipedia_langid.dev.tok.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Train a Naive Bayes classifier, you can make use of the scikit-learn implementation. See: [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB), note that there is are multiple variety of Naive Bayes implementations in sklearn, the one discussed in the book/slides is the multinomial variant.\n",
    "\n",
    "**Note**: the input is a list of lists of features `x` and a list of corresponding gold labels `y`. Therefore, the following should hold `len(x) == len(y)` and their indices should match.\n",
    "Additionally, every instance in `x` should have the same length (the number of features).\n",
    "\n",
    "b) Run the classifier on the dev data. It is crucial that you ensure that the feature values have exactly the same order as during training. How well does it perform? (accuracy?)\n",
    "\n",
    "**Note**: you cannot introduce new features here (!): you have to use the exact same features as the ones used during training.\n",
    "\n",
    "**Hint**: If the accuracy is lower than 50%, you are probably mixing up the feature order, either during training or during development or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Discriminative Classifier with BOW\n",
    "\n",
    "a) Train a `logistic regression` classifier in a similar fashion. For more information, see: [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Does it outperform the naive bayes classifier?\n",
    "\n",
    "b) Now evaluate both classifiers (`logistic regression` and `naive bayes`) on the out-of-domain Bulbapedia data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulba_dev_text, bulba_dev_labels = load_langid('langid-data/bulbapedia_langid.dev.tok.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are the trends similar to the Wookipedia data? Is there a performance drop compared to the Wookipedia data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Character n-grams\n",
    "Instead of using word unigrams as features, character n-grams can provide better generalization. \n",
    "\n",
    "a) Implement character tri-gram features without using the sklearn implementations.\n",
    "\n",
    "b) Train the `logistic regression` model on the tri-gram features and inspect performance on both the Wookipedia and Bulbapedia data. Does it outperform the BOW model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis\n",
    "There are two obvious ways to inspect the classifiers in more detail: by inspecting a confusion matrix and by\n",
    "examining the feature weights.\n",
    "\n",
    "### Confusion matrix\n",
    "a) Plot a confusion matrix for the logistic regression BOW model `6a)` when used on Bulbapedia data, and inspect the errors (it is not important how you visualize the results: a table, a figure, or even an ASCII table will suffice). \n",
    "Are there any interesting trends?\n",
    "\n",
    "### Feature weights\n",
    "In scikit-learn, you can inspect the internal weights given to each feature in the `.coef_` variable. Inspect the\n",
    "most important features for both classifiers. \n",
    "\n",
    "b) Are there any interesting differences?\n",
    "\n",
    "**Hint**: The weights are given per class, so you can either inspect three lists, or compute the average importance\n",
    "(make sure to use the absolute feature values for the average)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
