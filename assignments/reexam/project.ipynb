{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-exam Second Year Project (Introduction to Natural Language Processing and Deep Learning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re-exam for this course consists of a individual project (as opposed to the group project in the regular exam). \n",
    "For this project, you are going to compare the performance of two different models for the task of Named Entity Recognition (NER). \n",
    "\n",
    "During the course, we covered NLP models of varying complexity, in this project you are required to select two of these model types and you are going to compare them to each other. You should select (at least) two out of:\n",
    "* HMM (Viterbi algorithm)\n",
    "* RNN \n",
    "* BILSTM\n",
    "* BERT\n",
    "\n",
    "\n",
    "For each of the two models you pick, you should implement a baseline that scores at least 0.4 [span F1](https://github.itu.dk/robv/intro-nlp2023/tree/main/assignments/project/span_f1.py ) on the English Web Treebank data: https://github.itu.dk/robv/intro-nlp2023/tree/main/assignments/project/ewt-ner-normalized \n",
    "\n",
    "*Note that if you pick the Viterbi algorithm, it might be more challenging to do an interesting analysis, as it is much simpler compared to the others and it can be expected to underperform on almost all cases*\n",
    "\n",
    "For the comparison of the models, the starting point is comparing a single metric (e.g. span-f1), however the main contribution should be in the analysis part (see for suggestions below). Use at least two different analysis types.\n",
    "You can decide for yourself which dataset you prefer to use, for inspiration see: https://github.itu.dk/robv/intro-nlp2023/blob/main/slides/14-project.pdf \n",
    "\n",
    "\n",
    "The final project should have a size of 3-4 pages in [ACL format](https://github.com/acl-org/acl-style-files) (excluding bibliography and appendix). \n",
    "\n",
    "If you use a chatbot for your project, you have to include a report on usage of chatbots. We follow: https://2023.aclweb.org/blog/ACL-2023-policy/, so you have to:\n",
    "* Include a section in appendix (since we do not use a Responsible NLP Checklist)\n",
    "* Include each stage on the ACL policy, and indicate to what extent you used a chatbot\n",
    "* Use with care!, you are responsible for the project and plagiarism, correctness etc.\n",
    "* We would also strongly recommend to include a transcript of your interaction with the bot in the repository for reproducability reasons.\n",
    "\n",
    "In the appendix you can also put additional results and details. However, the paper itself should be standalone, and understandable without consulting the appendix.\n",
    "\n",
    "Furthermore, the code should be available on www.github.itu.dk (with a link in a footnote at the end of the abstract) , it should include a README with instructions on how to reproduce your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Analysis is essential for the interpretation of your results. In this section we will shortly describe some different types of analysis. We strongly suggest to use at least one of these:\n",
    "\n",
    "* **Ablation study**: Leave out a certain part of the model, to study its effects. For example, disable the tokenizer, remove a certain (group of) feature(s), or disable the stop-word removal. If the performance drops a lot, it means that this part of the model contributes heavily to the models final performance. This is commonly done in 1 table, while disabling different parts of the model. Note that you can also do this the other way around, i.e. use only one feature (group) at a time, and test performance\n",
    "* **Learning curve**: Evaluate how much data your model needs to reach a certain performance. Especially for the data augmentation projects this is essential.\n",
    "* **Quantitative analysis**: Automated means of analyzing in which cases your model performs worse. This can for example be done with a confusion matrix. Or looking at whether the weakness of the model is in finding the labels or the entity boundaries.\n",
    "* **Performance on subsets**: for example unseen entities (not in training data), or entities not starting with a capital etc. \n",
    "* **Qualitative analysis**: Manually inspect a certain number of errors, and try to categorize them/find trends. Can be combined with the quantitative analysis, i.e., inspect 100 cases of positive reviews predicted to be negative and 100 cases of negative reviews predicted to be positive\n",
    "* **Feature importance**: In traditional machine learning methods, one can often extract and inspect the weights of the features. In sklearn these can be found in: `trained_model.coef_`\n",
    "* **Input words importance**: To gain insight into which words have a impact on prediction performance (positive, negative), we can analyze per-word impact: given a trained model, replace a given word with\n",
    "the unknown word token and observe the change in prediction score (probability for a class). This is\n",
    "shown in Figure 4 of [Rethmeier et al (2018)](https://aclweb.org/anthology/W18-6246) (a paper on controversy detection), also shown below: red-colored\n",
    "tokens were important for controversy detection, blue-colored token decreased prediction scores.\n",
    "\n",
    "<img width=400px src=example.png>\n",
    "\n",
    "Note that this is a non-exhaustive list, and you are encouraged to also explore additional analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist final project\n",
    "Please check all these items before handing in your final report. You only have to upload a pdf file on learnit, and make sure a link to the code is included in the report and the code is accesible. \n",
    "\n",
    "* Are your name and email address on top of the first page?\n",
    "* Does the report include a representative project title?\n",
    "* Does the report contain an abstract?\n",
    "* Does the introduction clearly specify the research intention and research question?\n",
    "* Does the report adequately refer to the relevant literature?\n",
    "* Does the report properly use figure, tables and examples?\n",
    "* Does the report provide and discuss the empirical results?\n",
    "* Is the report proofread?\n",
    "* Does the pdf contain the link to the projectâ€™s github repo?\n",
    "* Is the github repo accessible to the public (within ITU)?\n",
    "* Is the report maximum 4 pages long, excluding references and appendix?\n",
    "* Does the repository contain all scripts and code to reproduce the results in the report? Are instructions\n",
    " provided on how to run the code?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
